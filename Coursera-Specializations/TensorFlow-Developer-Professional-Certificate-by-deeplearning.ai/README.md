<img align="right" width="100" height="100" src="https://github.com/cs-MohamedAyman/Coursera-Specializations/blob/master/organizations-logos/deeplearning.ai.jpg">

# [TensorFlow Developer Professional Certificate](https://www.coursera.org/professional-certificates/tensorflow-in-practice) `100H`

## WHAT YOU WILL LEARN
- Best practices for TensorFlow, a popular open-source machine learning framework to train a neural network for a computer vision applications.
- Handle real-world image data and explore strategies to prevent overfitting, including augmentation and dropout.
- Build natural language processing systems using TensorFlow.
- Apply RNNs, GRUs, and LSTMs as you train them using text repositories.

## SKILLS YOU WILL GAIN
`computer vision` `convolutional neural network` `machine learning` `natural language processing` `tensorflow` `inductive transfer` `augmentation` `dropouts` `tokenization` `rnns` `forecasting` `time series` `prediction`

## About this Specialization
- TensorFlow is one of the most in-demand and popular open-source deep learning frameworks available today. The DeepLearning.AI TensorFlow Developer Professional Certificate program teaches you applied machine learning skills with TensorFlow so you can build and train powerful models.

- In this hands-on, four-course Professional Certificate program, you’ll learn the necessary tools to build scalable AI-powered applications with TensorFlow. After finishing this program, you’ll be able to apply your new TensorFlow skills to a wide range of problems and projects. This program can help you prepare for the [Google TensorFlow Certificate exam](https://www.tensorflow.org/certificate) and bring you one step closer to achieving the Google TensorFlow Certificate.

- Looking for more advanced TensorFlow content? Check out the [TensorFlow: Data and Deployment Specialization](https://www.coursera.org/specializations/tensorflow-data-and-deployment).

## Applied Learning Project
In the DeepLearning.AI TensorFlow Developer Professional Certificate program, you'll get hands-on experience through 16 Python programming assignments. By the end of this program, you will be ready to:
- Build and train neural networks using TensorFlow
- Improve your network’s performance using convolutions as you train it to identify real-world images
- Teach machines to understand, analyze, and respond to human speech with natural language processing systems
- Process text, represent sentences as vectors, and train a model to create original poetry!

<details>
	<summary>Specialization Details</summary>

- If you are a software developer who wants to build scalable AI-powered algorithms, you need to understand how to use the tools to build them. This course is part of the upcoming Machine Learning in Tensorflow Specialization and will teach you best practices for using TensorFlow, a popular open-source framework for machine learning.
- The Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper understanding of how neural networks work, we recommend that you take the Deep Learning Specialization.
- In Course 2 of the deeplearning.ai TensorFlow Specialization, you will learn advanced techniques to improve the computer vision model you built in Course 1. You will explore how to work with real-world images in different shapes and sizes, visualize the journey of an image through convolutions to understand how a computer “sees” information, plot loss and accuracy, and explore strategies to prevent overfitting, including augmentation and dropout. Finally, Course 2 will introduce you to transfer learning and how learned features can be extracted from models. The Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper understanding of how neural networks work, we recommend that you take the Deep Learning Specialization.
- In Course 3 of the deeplearning.ai TensorFlow Specialization, you will build natural language processing systems using TensorFlow. You will learn to process text, including tokenizing and representing sentences as vectors, so that they can be input to a neural network. You’ll also learn to apply RNNs, GRUs, and LSTMs in TensorFlow. Finally, you’ll get to train an LSTM on existing text to create original poetry! The Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper understanding of how neural networks work, we recommend that you take the Deep Learning Specialization.
- In this fourth course, you will learn how to build time series models in TensorFlow. You’ll first implement best practices to prepare time series data. You’ll also explore how RNNs and 1D ConvNets can be used for prediction. Finally, you’ll apply everything you’ve learned throughout the Specialization to build a sunspot prediction model using real-world data! The Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper understanding of how neural networks work, we recommend that you take the Deep Learning Specialization.

</details>

## There are 4 Courses in this Specialization

## Course 1: [Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning](https://www.coursera.org/learn/introduction-tensorflow) `40H`

### Week 1: A New Programming Paradigm
```Welcome to this course on going from Basics to Mastery of TensorFlow. We're excited you're here! In week 1 you'll get a soft introduction to what Machine Learning and Deep Learning are, and how they offer you a new programming paradigm, giving you a new set of tools to open previously unexplored scenarios. All you need to know is some very basic programming skills, and you'll pick the rest up as you go along.```

```You'll be working with code that works well across both TensorFlow 1.x and the TensorFlow 2.0 alpha. To get started, check out the first video, a conversation between Andrew and Laurence that sets the theme for what you'll study.```

<details>
      <summary>Week Details</summary>
<br>

- A new programming paradigm
  - Reading: Before you begin: TensorFlow 2.0 and this course
  - Video: Introduction: A conversation with Andrew Ng
  - Video: A primer in machine learning
  - Video: The ‘Hello World’ of neural networks
  - Reading: From rules to data
  - Video: Working through ‘Hello World’ in TensorFlow and Python
  - Reading: Try it for yourself
  - Quiz: Week 1 Quiz
- Weekly Exercise - Your First Neural Network
  - Reading: Introduction to Google Colaboratory
  - Get started with Google Colaboratory (Coding TensorFlow)
  - Lab: Exercise 1 (Housing Prices)
- Programming Assignment: Exercise 1 (Housing Prices)
  - Reading: Week 1 Resources
- Optional: Ungraded Google Colaboratory environment
  - Ungraded External Tool: Ungraded External ToolExercise 1 (Housing Prices)
</details>

### Week 2: Introduction to Computer Vision
```Welcome to week 2 of the course! In week 1 you learned all about how Machine Learning and Deep Learning is a new programming paradigm. This week you’re going to take that to the next level by beginning to solve problems of computer vision with just a few lines of code! Check out this conversation between Laurence and Andrew where they discuss it and introduce you to Computer Vision!```

<details>
      <summary>Week Details</summary>
<br>

- Introduction to Computer Vision
  - Video: A Conversation with Andrew Ng
  - Video: An Introduction to computer vision
  - Reading: Exploring how to use data
  - Video: Writing code to load training data
  - Reading: The structure of Fashion MNIST data
  - Video: Coding a Computer Vision Neural Network
  - Reading: See how it's done
  - Video: Walk through a Notebook for computer vision
  - Reading: Get hands-on with computer vision
  - Video: Using Callbacks to control training
  - Reading: See how to implement Callbacks
  - Video: Walk through a notebook with Callbacks
  - Quiz: Week 2 Quiz
- Weekly Exercise - Implement a Deep Neural Network to recognize handwritten digits
  - Lab: Exercise 2 (Handwriting Recognition)
- Programming Assignment: Exercise 2
  - Reading: Week 2 Resources
- Optional: Ungraded Google Colaboratory environment
  - Ungraded External Tool: Ungraded External ToolExercise 2 (Handwriting Recognition)
</details>

### Week 3: Enhancing Vision with Convolutional Neural Networks
```Welcome to week 3! In week 2 you saw a basic Neural Network for Computer Vision. It did the job nicely, but it was a little naive in its approach. This week we’ll see how to make it better, as discussed by Laurence and Andrew here.```

<details>
      <summary>Week Details</summary>
<br>

- Enhancing Vision with Convolutional Neural Networks
  - Video: A conversation with Andrew Ng
  - Video: What are convolutions and pooling?
  - Reading: Coding convolutions and pooling layers
  - Video: Implementing convolutional layers
  - Reading: Learn more about convolutions
  - Video: Implementing pooling layers
  - Reading: Getting hands-on, your first ConvNet
  - Video: Improving the Fashion classifier with convolutions
  - Reading: Try it for yourself
  - Video: Walking through convolutions
  - Reading: Experiment with filters and pools
  - Quiz: Week 3 Quiz
- Weekly Exercise - Improving DNN Performance using Convolutions
  - Lab: Exercise 3 (Improve MNIST with convolutions)
  - Programming Assignment: Exercise 3 (Improve MNIST with convolutions))
  - Reading: Week 3 Resources
- Optional: Ungraded Google Colaboratory environment
  - Ungraded External Tool: Ungraded External ToolExercise 3 - Improve MNIST with convolutions
</details>

### Week 4: Using Real-world Images
```Last week you saw how to improve the results from your deep neural network using convolutions. It was a good start, but the data you used was very basic. What happens when your images are larger, or if the features aren’t always in the same place? Andrew and Laurence discuss this to prepare you for what you’ll learn this week: handling complex images!```

<details>
      <summary>Week Details</summary>
<br>

- Using Real-world Images
  - Video: A conversation with Andrew Ng
  - Reading: Explore an impactful, real-world solution
  - Video: Understanding ImageGenerator
  - Reading: Designing the neural network
  - Video: Defining a ConvNet to use complex images
  - Reading: Train the ConvNet with ImageGenerator
  - Video: Training the ConvNet with fit_generator
  - Reading: Exploring the solution
  - Video: Walking through developing a ConvNet
  - Reading: Training the neural network
  - Video: Walking through training the ConvNet with fit_generator
  - Reading: Experiment with the horse or human classifier
  - Video: Adding automatic validation to test accuracy
  - Reading: Get hands-on and use validation
  - Video: Exploring the impact of compressing images
  - Reading: Get Hands-on with compacted images
  - Quiz: Week 4 Quiz
- Weekly Exercise - Handling Complex Images
  - Lab: Exercise 4 (Handling complex images)
- Programming Assignment: Exercise 4 (Handling complex images)
  - Reading: Week 4 Resources
- Optional: Ungraded Google Colaboratory environment
  - Ungraded External Tool: Ungraded External ToolExercise 4 - Handling complex images
- Course 1 Wrap up
  - Reading: Wrap up
  - Video: A conversation with Andrew
</details>

## Course 2: [Convolutional Neural Networks in TensorFlow](https://www.coursera.org/learn/convolutional-neural-networks-tensorflow) `30H`

### Week 1: Exploring a Larger Dataset
```In the first course in this specialization, you had an introduction to TensorFlow, and how, with its high level APIs you could do basic image classification, an you learned a little bit about Convolutional Neural Networks (ConvNets). In this course you'll go deeper into using ConvNets will real-world data, and learn about techniques that you can use to improve your ConvNet performance, particularly when doing image classification!```

```In Week 1, this week, you'll get started by looking at a much larger dataset than you've been using thus far: The Cats and Dogs dataset which had been a Kaggle Challenge in image classification!```

<details>
      <summary>Week Details</summary>
<br>

- Introduction
  - Video: Introduction, A conversation with Andrew Ng
  - Reading: Before you Begin: TensorFlow 2.0 and this Course
- Larger Dataset
  - Video: A conversation with Andrew Ng
  - Reading: The cats vs dogs dataset
  - Video: Training with the cats vs. dogs dataset
  - Reading: Looking at the notebook
  - Video: Working through the notebook
  - Reading: What you'll see next
  - Video: Fixing through cropping
  - Video: Visualizing the effect of the convolutions
  - Video: Looking at accuracy and loss
  - Reading: What have we seen so far?
  - Quiz: Week 1 Quiz
  - Video: Week 1 Wrap up
- Weekly Exercise- Attempt the cats vs. dogs Kaggle challenge!
  - Ungraded External Tool: Ungraded External ToolExercise 1 - Cats vs. Dogs
  - Ungraded External Tool: Ungraded External ToolExercise 1 Answer- Cats vs. Dogs
</details>

### Week 2: Augmentation: A technique to avoid overfitting
```You've heard the term overfitting a number of times to this point. Overfitting is simply the concept of being over specialized in training -- namely that your model is very good at classifying what it is trained for, but not so good at classifying things that it hasn't seen. In order to generalize your model more effectively, you will of course need a greater breadth of samples to train it on. That's not always possible, but a nice potential shortcut to this is Image Augmentation, where you tweak the training set to potentially increase the diversity of subjects it covers. You'll learn all about that this week!```

<details>
      <summary>Week Details</summary>
<br>

- Augmentation
  - Video: A conversation with Andrew Ng
  - Reading: Image Augmentation
  - Video: Introducing augmentation
  - Reading: Start Coding...
  - Video: Coding augmentation with ImageDataGenerator
  - Reading: Looking at the notebook
  - Video: Demonstrating overfitting in cats vs. dogs
  - Reading: The impact of augmentation on Cats vs. Dogs
  - Video: Adding augmentation to cats vs. dogs
  - Reading: Try it for yourself!
  - Video: Exploring augmentation with horses vs. humans
  - Reading: What have we seen so far?
  - Quiz: Week 2 Quiz
  - Video: Week 2 Wrap up
- Weekly Exercise- Full cats vs. dogs using augmentation
  - Ungraded External Tool: Ungraded External ToolExercise 2 - Cats vs. Dogs using augmentation
  - Ungraded External Tool: Ungraded External ToolExercise 2 Answer - Cats vs. Dogs using augmentation
</details>

### Week 3: Transfer Learning
```Building models for yourself is great, and can be very powerful. But, as you've seen, you can be limited by the data you have on hand. Not everybody has access to massive datasets or the compute power that's needed to train them effectively. Transfer learning can help solve this -- where people with models trained on large datasets train them, so that you can either use them directly, or, you can use the features that they have learned and apply them to your scenario. This is Transfer learning, and you'll look into that this week!```

<details>
      <summary>Week Details</summary>
<br>

- Transfer Learning
  - Video: A conversation with Andrew Ng
  - Video: Understanding transfer learning: the concepts
  - Reading: Start coding!
  - Video: Coding transfer learning from the inception mode
  - Reading: Adding your DNN
  - Video: Coding your own model with transferred features
  - Reading: Using dropouts!
  - Video: Exploring dropouts
  - Reading: Applying Transfer Learning to Cats v Dogs
  - Video: Exploring Transfer Learning with Inception
  - Reading: What have we seen so far?
  - Quiz: Week 3 Quiz
  - Video: Week 3 Wrap up
- Weekly Exercise- Transfer Learning
  - Ungraded External Tool: Ungraded External ToolExercise 3 - Horses vs. humans using Transfer Learning
  - Ungraded External Tool: Ungraded External ToolExercise 3 Answer - Horses vs. humans using Transfer Learning
</details>

### Week 4: Multiclass Classifications
```You've come a long way, Congratulations! One more thing to do before we move off of ConvNets to the next module, and that's to go beyond binary classification. Each of the examples you've done so far involved classifying one thing or another -- horse or human, cat or dog. When moving beyond binary into Categorical classification there are some coding considerations you need to take into account. You'll look at them this week!```

<details>
      <summary>Week Details</summary>
<br>

- Multiclass Classifications
  - Video: A conversation with Andrew Ng
  - Video: Moving from binary to multi-class classification
  - Reading: Introducing the Rock-Paper-Scissors dataset
  - Video: Explore multi-class with Rock Paper Scissors dataset
  - Reading: Check out the code!
  - Video: Train a classifier with Rock Paper Scissors
  - Reading: Try testing the classifier
  - Video: Test the Rock Paper Scissors classifier
  - Reading: What have we seen so far?
  - Quiz: Week 4 Quiz
- Weekly Exercise- Multi-class classifier
  - Ungraded External Tool: Ungraded External ToolExercise 4 - Multi-class classifier
  - Ungraded External Tool: Ungraded External ToolExercise 4 Answer- Multi-class classifier
- Course 2 Wrap up
  - Reading: Wrap up
  - Video: A conversation with Andrew Ng
</details>

## Course 3: [Natural Language Processing in TensorFlow](https://www.coursera.org/learn/natural-language-processing-tensorflow) `15H`

### Week 1: Sentiment in text
```The first step in understanding sentiment in text, and in particular when training a neural network to do so is the tokenization of that text. This is the process of converting the text into numeric values, with a number representing a word or a character. This week you'll learn about the Tokenizer and pad_sequences APIs in TensorFlow and how they can be used to prepare and encode text and sentences to get them ready for training neural networks!```

<details>
      <summary>Week Details</summary>
<br>

- Introduction
  - Video: Introduction, A conversation with Andrew Ng
- Sentiment in text
  - Video: Introduction
  - Video: Word based encodings
  - Video: Using APIs
  - Reading: Check out the code!
  - Video: Notebook for lesson 1
  - Video: Text to sequence
  - Video: Looking more at the Tokenizer
  - Video: Padding
  - Reading: Check out the code!
  - Video: Notebook for lesson 2
  - Video: Sarcasm, really?
  - Video: Working with the Tokenizer
  - Reading: News headlines dataset for sarcasm detection
  - Reading: Check out the code!
  - Video: Notebook for lesson 3
  - Quiz: Week 1 Quiz
  - Video: Week 1 Wrap up
- Weekly Exercise- Explore the BBC News Archive
  - Ungraded External Tool: Ungraded External ToolExercise 1- Explore the BBC news archive
  - Ungraded External Tool: Ungraded External ToolExercise 1 Answer- Explore the BBC news archive
</details>

### Week 2: Word Embeddings
```Last week you saw how to use the Tokenizer to prepare your text to be used by a neural network by converting words into numeric tokens, and sequencing sentences from these tokens. This week you'll learn about Embeddings, where these tokens are mapped as vectors in a high dimension space. With Embeddings and labelled examples, these vectors can then be tuned so that words with similar meaning will have a similar direction in the vector space. This will begin the process of training a neural network to udnerstand sentiment in text -- and you'll begin by looking at movie reviews, training a neural network on texts that are labelled 'positive' or 'negative' and determining which words in a sentence drive those meanings.```

<details>
      <summary>Week Details</summary>
<br>

- Word Embeddings
  - Video: A conversation with Andrew Ng
  - Video: Introduction
  - Video: The IMBD dataset
  - Reading: IMDB reviews dataset
  - Video: Looking into the details
  - Video: How can we use vectors?
  - Video: More into the details
  - Reading: Check out the code!
  - Video: Notebook for lesson 1
  - Video: Remember the sarcasm dataset?
  - Video: Building a classifier for the sarcasm dataset
  - Video: Let’s talk about the loss function
  - Reading: Check out the code!
  - Video: Pre-tokenized datasets
  - Reading: TensorFlow datasets
  - Video: Diving into the code (part 1)
  - Reading: Subwords text encoder
  - Video: Diving into the code (part 2)
  - Reading: Check out the code!
  - Video: Notebook for lesson 3
  - Quiz: Week 2 Quiz
  - Reading: Week 2 Wrap up
- Weekly Exercise- More on the BBC News Archive
  - Ungraded External Tool: Ungraded External ToolExercise 2- BBC news archive
  - Ungraded External Tool: Ungraded External ToolExercise 2 Answer- BBC news archive
</details>

### Week 3: Sequence models
```In the last couple of weeks you looked first at Tokenizing words to get numeric values from them, and then using Embeddings to group words of similar meaning depending on how they were labelled. This gave you a good, but rough, sentiment analysis -- words such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one. But sentiment can also be determined by the sequence in which words appear. For example, you could have 'not fun', which of course is the opposite of 'fun'. This week you'll start digging into a variety of model formats that are used in training models to understand context in sequence!```

<details>
      <summary>Week Details</summary>
<br>

- Sequence models
  - Video: A conversation with Andrew Ng
  - Video: Introduction
  - Reading: Link to Andrew's sequence modeling course
  - Video: LSTMs
  - Reading: More info on LSTMs
  - Video: Implementing LSTMs in code
  - Reading: Check out the code!
  - Video: Accuracy and loss
  - Video: A word from Laurence
  - Video: Looking into the code
  - Video: Using a convolutional network
  - Reading: Check out the code!
  - Video: Going back to the IMDB dataset
  - Reading: Check out the code!
  - Video: Tips from Laurence
  - Reading: Exploring different sequence models
  - Quiz: Week 3 Quiz
  - Reading: Week 3 Wrap up
- Weekly Exercise- Exploring overfitting in NLP
  - Ungraded External Tool: Ungraded External ToolExercise 3- Exploring overfitting in NLP
  - Ungraded External Tool: Ungraded External ToolExercise 3 Answer- Exploring overfitting in NLP
</details>

### Week 4: Sequence models and literature
```Taking everything that you've learned in training a neural network based on NLP, we thought it might be a bit of fun to turn the tables away from classification and use your knowledge for prediction. Given a body of words, you could conceivably predict the word most likely to follow a given word or phrase, and once you've done that, to do it again, and again. With that in mind, this week you'll build a poetry generator. It's trained with the lyrics from traditional Irish songs, and can be used to produce beautiful-sounding verse of it's own!```

<details>
      <summary>Week Details</summary>
<br>

- Sequence models and literature
  - Video: A conversation with Andrew Ng
  - Video: Introduction
  - Video: Looking into the code
  - Video: Training the data
  - Video: More on training the data
  - Reading: Check out the code!
  - Video: Notebook for lesson 1
  - Video: Finding what the next word should be
  - Video: Example
  - Video: Predicting a word
  - Video: Poetry!
  - Reading: link to Laurence's poetry
  - Video: Looking into the code
  - Video: Laurence the poet!
  - Reading: Check out the code!
  - Video: Your next task
  - Reading: Link to generating text using a character-based RNN
  - Quiz: Week 4 Quiz
- Weekly Exercise- Using LSTMs, see if you can write Shakespeare!
  - Ungraded External Tool: Ungraded External ToolExercise 4- Using LSTMs, see if you can write Shakespeare!
  - Ungraded External Tool: Ungraded External ToolExercise 4 Answer- Using LSTMs, see if you can write Shakespeare!
- Course 3 Wrap up
  - Reading: Wrap up
  - Video: A conversation with Andrew Ng
</details>

## Course 4: [Sequences, Time Series and Prediction](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction) `15H`

### Week 1: Sequences and Prediction
```Hi Learners and welcome to this course on sequences and prediction! In this course we'll take a look at some of the unique considerations involved when handling sequential time series data -- where values change over time, like the temperature on a particular day, or the number of visitors to your web site. We'll discuss various methodologies for predicting future values in these time series, building on what you've learned in previous courses!```

<details>
      <summary>Week Details</summary>
<br>

- Introduction
  - Video: Introduction, A conversation with Andrew Ng
- Sequences and Prediction
  - Video: Time series examples
  - Video: Machine learning applied to time series
  - Video: Common patterns in time series
  - Video: Introduction to time series
  - Reading: Introduction to time series notebook
  - Video: Train, validation and test sets
  - Video: Metrics for evaluating performance
  - Video: Moving average and differencing
  - Video: Trailing versus centered windows
  - Video: Forecasting
  - Reading: Forecasting notebook
  - Quiz: Week 1 Quiz
  - Reading: Week 1 Wrap up
- Weekly Exercise-Create and predict synthetic data
  - Ungraded External Tool: Ungraded External ToolExercise 1 - Create and predict synthetic data
  - Ungraded External Tool: Ungraded External ToolExercise 1 Answer- Create and predict synthetic data
</details>

### Week 2: Deep Neural Networks for Time Series
```Having explored time series and some of the common attributes of time series such as trend and seasonality, and then having used statistical methods for projection, let's now begin to teach neural networks to recognize and predict on time series!```

<details>
      <summary>Week Details</summary>
<br>

- Deep Neural Networks for Time Series
  - Video: A conversation with Andrew Ng
  - Video: Preparing features and labels
  - Video: Preparing features and labels
  - Reading: Preparing features and labels notebook
  - Reading: Sequence bias
  - Video: Feeding windowed dataset into neural network
  - Video: Single layer neural network
  - Video: Machine learning on time windows
  - Video: Prediction
  - Video: More on single layer neural network
  - Reading: Single layer neural network notebook
  - Video: Deep neural network training, tuning and prediction
  - Video: Deep neural network
  - Reading: Deep neural network notebook
  - Quiz: Week 2 Quiz
  - Reading: Week 2 Wrap up
- Weekly Exercise - Create Synthetic Data and predict with a DNN
  - Ungraded External Tool: Ungraded External ToolExercise 2 - Predict with a DNN
  - Ungraded External Tool: Ungraded External ToolExercise 2 Answer- Predict with a DNN
</details>

### Week 3: Recurrent Neural Networks for Time Series
```Recurrent Neural networks and Long Short Term Memory networks are really useful to classify and predict on sequential data. This week we'll explore using them with time series.```

<details>
      <summary>Week Details</summary>
<br>

- Recurrent Neural Networks for time series
  - Video: Week 3 - A conversation with Andrew Ng
  - Video: Conceptual overview
  - Video: Shape of the inputs to the RNN
  - Video: Outputting a sequence
  - Video: Lambda layers
  - Video: Adjusting the learning rate dynamically
  - Reading: More info on Huber loss
  - Video: RNN
  - Reading: RNN notebook
  - Video: LSTM
  - Reading: Link to the LSTM lesson
  - Video: Coding LSTMs
  - Video: More on LSTM
  - Reading: LSTM notebook
  - Quiz: Week 3 Quiz
  - Reading: Week 3 Wrap up
- Weekly Exercise- Mean Absolute Error
  - Ungraded External Tool: Ungraded External ToolExercise 3 - Mean Absolute Error
  - Ungraded External Tool: Ungraded External ToolExercise 3 Answer - Mean Absolute Error
</details>

### Week 4: Real-world time series data
```On top of DNNs and RNNs, let's also add convolutions, and then put it all together using a real-world data series -- one which measures sunspot activity over hundreds of years, and see if we can predict using it.```

<details>
      <summary>Week Details</summary>
<br>

- Real-world time series data
  - Video: Week 4 - A conversation with Andrew Ng
  - Video: Convolutions
  - Reading: Convolutional neural networks course
  - Video: Bi-directional LSTMs
  - Reading: More on batch sizing
  - Video: LSTM
  - Reading: LSTM notebook
  - Video: Real data - sunspots
  - Video: Train and tune the model
  - Video: Prediction
  - Video: Sunspots
  - Reading: Sunspots notebook
  - Video: Combining our tools for analysis
  - Quiz: Week 4 Quiz
- Weekly Exercise - Sunspots
  - Ungraded External Tool: Ungraded External ToolExercise 4 - Sunspots
  - Ungraded External Tool: Ungraded External ToolExercise 4 Answer - Sunspots
- Course 4 Wrap up
  - Reading: Wrap up
  - Video: Congratulations!
- TensorFlow in practice has come to an end
  - Video: Specialization wrap up - A conversation with Andrew Ng
</details>
